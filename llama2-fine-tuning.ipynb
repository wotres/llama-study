{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ed179-aa8a-4644-b6ba-9fd7026e9bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 한줄로 파인튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd1e52-819b-429b-91d5-f4c35a241ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q autotrain-advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7dbd98-0b57-4884-8fdb-be55d32f7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!autotrain llm --train \\\n",
    "    --project_name \"llama2-finetune\" \\\n",
    "    --model \"Llama-2-7B\" \\\n",
    "    --data_path \"royboy0416/ko-alpaca\" \\\n",
    "    --text_column \"text\" \\\n",
    "    --use_peft \\\n",
    "    --use_int4 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --train_batch_size 2 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --trainer sft \\\n",
    "    --model_max_length 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98291507-f094-40e6-9c88-f9de0eed0c78",
   "metadata": {},
   "source": [
    "project_name : 저장될 폴더이름\n",
    "model : 사용할 모델\n",
    "data_path: 허깅 페이스에 올라온 데이터 경로 ex) royboy0416/ko-alpaca\n",
    "text_column: 데이터에서 사용할 컬럼\n",
    "use_peft: parameter efficient fine tuning => 모든 파라미터를 동등하게 취급하는 것이 아니라, 특정 작업에서 중요한 파라미터만 업데이트하는 것\n",
    "use_int4: 4비트 정수 사용 => 메모리 사용량 줄이고 계산 효율성을 높임\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c7aec-1162-4b90-9b27-523764e48079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코랩 사용시 데이터 옮기는 법\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "!cp -a /content/llama2-finetune /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf2c7d8-ee58-4752-bcae-578603fb09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd9b06-6b36-45c2-89fb-ff7133cbc744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드 후 사용 (README 에 있던 값 사용)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_id = \"Llama-2-7B\"\n",
    "peft_model_id = \"/content/drive/MyDrive/llama2-finetune/checkpoint-1111\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None,\n",
    "    llm_int8_enable_fp32_cpu_offload=False,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b5310-2300-4693-81f0-dc180d805bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: %s ### Response: \"\n",
    "\n",
    "def generate_answer(question):\n",
    "    prompt_question = prompt % (question,)\n",
    "    answer = model.generate(\n",
    "        **tokenizer(\n",
    "            prompt_question,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False\n",
    "        ).to('cuda'),\n",
    "        max_new_tokens=128,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    return tokenizer.decode(answer[0]).replace(prompt_question, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70656fc2-fee2-4980-b456-d9006dd724f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\"hi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
